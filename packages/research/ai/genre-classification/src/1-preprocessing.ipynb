{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "This notebook has been run on my laptop, such that a dataset with spectrograms and genres from my\n",
    "personal music library has been created.\n",
    "\n",
    "## 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import tensor\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, Audio, ClassLabel, Features, Value, Array2D\n",
    "from transformers import ASTFeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Music Paths\n",
    "\n",
    "My music library consists of multiple top-level directories, like \"Big Room\", \"Techno\",\n",
    "\"Classic\", etc., which define the genre of the songs inside them. Each of these directories can\n",
    "have multiple subdirectories, e.g. for CDs, but the genre remains the same. Some folders like my own\n",
    "songs (\"Eigenes\"), audio books (\"Hörbücher\"), and mixes (\"Mixes\") are excluded from the dataset.\n",
    "\n",
    "Each folder needs at least 6 songs for the dataset to be used for training because the split into\n",
    "train, validate, and test sets requires at least one song per genre in each set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_lib: dict[str, list[str]] = {}\n",
    "def add_songs(dir: str, skip_dirs: list[str] = [], genre: str = \"\"):\n",
    "    for dirent in os.listdir(dir): \n",
    "        path = os.path.join(dir, dirent)\n",
    "        if os.path.isdir(path) and dirent not in skip_dirs:\n",
    "            add_songs(path, genre=genre if genre else dirent)\n",
    "        elif os.path.isfile(path) and path.endswith(\".mp3\"):\n",
    "            if genre not in music_lib:\n",
    "                music_lib[genre] = []\n",
    "            music_lib[genre].append(path)\n",
    "\n",
    "add_songs(\"D:\\\\Music\", [\"$RECYCLE.BIN\", \"downloads\", \"Eigenes\", \"Hörbücher\", \"Mixes\", \"System Volume Information\"])\n",
    "for genre, songs in list(music_lib.items()):\n",
    "    if len(songs) < 6: # at leat one in each dataset \n",
    "        del music_lib[genre]\n",
    "    else:\n",
    "        print(f\"{genre}: {len(songs)} songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, that the classes are highly unbalanced as I prefer some genres over others. E.g.\n",
    "\"Hands Up\" has over 1373 songs while \"Folk\" only has 6 (the minimum).\n",
    "\n",
    "The following genres are the under- and over-represented ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = { genre: len(songs) for genre, songs in music_lib.items() }\n",
    "treshold25 = tensor(list(lengths.values()), dtype=torch.float).quantile(0.25).item()\n",
    "treshold75 = tensor(list(lengths.values()), dtype=torch.float).quantile(0.75).item()\n",
    "\n",
    "print(\"Under-represented\", [genre for genre, length in lengths.items() if length < treshold25])\n",
    "print(\"Over-represented\", [genre for genre, length in lengths.items() if length > treshold75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(lengths.keys(), lengths.values())\n",
    "plt.title(\"Distribution of number of songs per genre\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, I have a favorite genre :D\n",
    "\n",
    "From the loaded music dict, a list of genres, their respective index (label), and file paths are\n",
    "created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = list(music_lib.keys())\n",
    "filepaths: list[str] = []\n",
    "labels: list[int] = []\n",
    "for genre_idx, songs in enumerate(music_lib.values()):\n",
    "    for filepath in songs:\n",
    "        filepaths.append(filepath)\n",
    "        labels.append(genre_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the genres for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/genres.json\", \"w+\", encoding=\"utf-8\") as f:\n",
    "    json.dump(genres, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Create an audio dataset\n",
    "\n",
    "Load the pre-trained model `MIT/ast-finetuned-audioset-10-10-0.4593` to get the name of the\n",
    "input column and define other column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "labels_name = \"labels\"\n",
    "paths_name = \"paths\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a function to hash the music dict is provided because HuggingFace datasets support setting\n",
    "fingerprints to load from cache. This way, the same splits are used when nothing changed in the\n",
    "music library, otherwise new splits are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_dict(d):\n",
    "    dict_tuple = tuple(sorted(d.items()))\n",
    "    return hashlib.sha256(repr(dict_tuple).encode()).hexdigest()[:48]\n",
    "\n",
    "music_lib_hash = hash_dict(music_lib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is created with features of `Audio` for the audio files and `ClassLabel` for their\n",
    "labels. To avoid loading the songs during the dataset split, temporary features are used at first\n",
    "with a simple string value for the song path.\n",
    "\n",
    "The split is then done twice. First with 70 % train and 30 % test, of which one third is used for\n",
    "validation and two third for the actual test set. After that, all three datasets are merged into one\n",
    "and the dataset is cast to the final features. The `stratify_bo_column` options allows the dataset\n",
    "to contain a balanced of genres, such that each genre has at least one song in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\n",
    "    model_input_name: Audio(sampling_rate=feature_extractor.sampling_rate),\n",
    "    labels_name: ClassLabel(names=genres)\n",
    "})\n",
    "\n",
    "tmp_features = Features({\n",
    "    model_input_name: Value(\"string\"),\n",
    "    labels_name: ClassLabel(names=genres)\n",
    "})\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    model_input_name: filepaths,\n",
    "    labels_name: labels,\n",
    "}, features=tmp_features).train_test_split(\n",
    "    test_size=0.3,\n",
    "    shuffle=True,\n",
    "    stratify_by_column=labels_name,\n",
    "    load_from_cache_file=True,\n",
    "    train_indices_cache_file_name=\"../cache/train_indices\",\n",
    "    test_indices_cache_file_name=\"../cache/validate_test_indices\",\n",
    "    train_new_fingerprint=f\"train_{music_lib_hash}\",\n",
    "    test_new_fingerprint=f\"validate_test_{music_lib_hash}\"\n",
    ")\n",
    "\n",
    "validation_dataset = dataset[\"test\"].train_test_split(\n",
    "    test_size=2/3,\n",
    "    shuffle=True,\n",
    "    stratify_by_column=labels_name,\n",
    "    load_from_cache_file=True,\n",
    "    train_indices_cache_file_name=\"../cache/validate_indices\",\n",
    "    test_indices_cache_file_name=\"../cache/test_indices\",\n",
    "    train_new_fingerprint=f\"validate_{music_lib_hash}\",\n",
    "    test_new_fingerprint=f\"test_{music_lib_hash}\"\n",
    ")\n",
    "\n",
    "dataset[\"validate\"] = validation_dataset[\"train\"]\n",
    "dataset[\"test\"] = validation_dataset[\"test\"]\n",
    "\n",
    "dataset = dataset.cast(features)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Create a spectrogram dataset\n",
    "\n",
    "Next, a function to create spectrograms is provided. This function gets an `Audio` object as input\n",
    "and uses the pre-defined parameters `sample_len_sec` and `samples_per_song` to create multiple\n",
    "spectrograms from the song with a given length. Since the AST model is trained on 10.24 seconds\n",
    "of audio, this is the default value.\n",
    "\n",
    "At first, I thought that the feature extractor would create a spectrogram from the whole song. Thus,\n",
    "my first dataset contained only one spectrogram per song and only of the first ten seconds. Using it\n",
    "to fine-tune the model led to a disappointing result of around 55 % accuracy. During the analysis, I\n",
    "saw how the feature extractor really works. Then, I decided to take to use snippets of 5 seconds at\n",
    "the beginning and after every 30 seconds of a song, but 5 at max. This led to a variable number of\n",
    "snippets per song due to different lengths. But using a different time than 10.24 s leads to\n",
    "modifications of the model config. I had to change the `max_length` field to be able to process\n",
    "another snippet length. I didn't realize, that this caused the model to ignore the pre-trained\n",
    "weights of the position embeddings, which I had to train anew then. The best model got a performance\n",
    "of around 79 %. When using the pre-trained length, everything worked far better. The training\n",
    "converged faster, and led to a better accuracy. Also, I was using 6 snippets per song, equally\n",
    "distributed over the length of a song, which enabled an easier creation of a model working with\n",
    "whole songs. The best model had then an accuracy of around 87.5 %.\n",
    "\n",
    "Visualizations of the results are inside the [results-notebook](./6-results.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_len_sec = 10.24\n",
    "samples_per_song = 6\n",
    "def preprocess_song(song):\n",
    "    audio = song[model_input_name]\n",
    "    sampling_rate = audio[\"sampling_rate\"]\n",
    "    wav = audio[\"array\"]\n",
    "    song_spetrograms = []\n",
    "    song_labels = []\n",
    "    song_paths = []\n",
    "    for start_sample in range(0, len(wav) - len(wav) % samples_per_song, len(wav) // samples_per_song):\n",
    "        end_sample = start_sample + round(sample_len_sec * sampling_rate)\n",
    "        input_wav = wav[start_sample:end_sample]\n",
    "        song_spetrograms.append(feature_extractor(input_wav, sampling_rate=sampling_rate, return_tensors=\"pt\")[model_input_name][0])\n",
    "        song_labels.append(song[labels_name])\n",
    "        song_paths.append(audio[\"path\"])\n",
    "    \n",
    "    return song_spetrograms, song_labels, song_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small helper function to visualize the spectrograms. You will find it in several places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_spectrum(specs, size=(10,6), cols=1, rows=1):\n",
    "    plt.figure(figsize=size)\n",
    "    for idx, spec in enumerate(specs):\n",
    "        plt.subplot(rows, cols, idx + 1)\n",
    "        plt.imshow(spec.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "        plt.colorbar(label=\"Amplitude\")\n",
    "        plt.xlabel(\"Time Frames\")\n",
    "        plt.ylabel(\"Frequency Bins\")\n",
    "        plt.tight_layout()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can for example look at the spectrogram of the first song in the train split using the\n",
    "`preprocess_song` function. Since one song has six spectrograms, we can choose a greater plot size\n",
    "and set the cols to 2 and the rows to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs, labels, paths = preprocess_song(dataset[\"train\"][0])\n",
    "print(paths[0])\n",
    "visualize_spectrum(specs, size=(20, 12), cols=2, rows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the spectrograms of a \"Drum and Bass\" song. You can clearly see where drums are playing\n",
    "and where not.\n",
    "\n",
    "Now we can preprocess the whole dataset using the `preprocess_song` function. The new dataset\n",
    "features now consist of a 2D array for the spectrograms, the song label, and the song path. The\n",
    "preprocessing has to be done in batches such that the process does not run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_features = Features({\n",
    "    model_input_name: Array2D(shape=(feature_extractor.max_length, feature_extractor.num_mel_bins), dtype=\"float32\"),\n",
    "    labels_name: ClassLabel(names=genres),\n",
    "    paths_name: Value(dtype=\"string\")\n",
    "})\n",
    "\n",
    "for split, sub_dataset in dataset.items():\n",
    "    batch_size = 2_000\n",
    "    batch_spectrograms = []\n",
    "    batch_labels = []\n",
    "    batch_paths = []\n",
    "    def save_batch(idx: int):\n",
    "        batch_dict = {model_input_name: batch_spectrograms, labels_name: batch_labels, paths_name: batch_paths}\n",
    "        partial_dataset = Dataset.from_dict(batch_dict, features=spectrogram_features)\n",
    "        partial_dataset.save_to_disk(f\"../spectrums/{split}_batch_{idx}\")\n",
    "\n",
    "    idx = 0\n",
    "    for song in tqdm(sub_dataset, desc=f\"Preparing {split} spectrums\", total=len(sub_dataset)):\n",
    "        song_spectrograms, song_labels, song_paths = preprocess_song(song)\n",
    "        batch_spectrograms = [*batch_spectrograms, *song_spectrograms]\n",
    "        batch_labels = [*batch_labels, *song_labels]\n",
    "        batch_paths = [*batch_paths, *song_paths]\n",
    "        if len(batch_spectrograms) >= batch_size:\n",
    "            save_batch(idx)\n",
    "            batch_spectrograms = []\n",
    "            batch_labels = []\n",
    "            batch_paths = []\n",
    "            idx += 1\n",
    "    save_batch(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the spectrograms from disk using the `Dataset.load_from_disk` function, concatenate\n",
    "them into a final preprocessed dataset, and save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_paths = [f\"../spectrums/{batch_dir}\" for batch_dir in os.listdir(\"../spectrums\")]\n",
    "\n",
    "train_batches = [Dataset.load_from_disk(path) for path in batch_paths if \"train\" in path]\n",
    "validate_batches = [Dataset.load_from_disk(path) for path in batch_paths if \"validate\" in path]\n",
    "test_batches = [Dataset.load_from_disk(path) for path in batch_paths if \"test\" in path]\n",
    "\n",
    "train_dataset = concatenate_datasets(train_batches)\n",
    "validate_dataset = concatenate_datasets(validate_batches)\n",
    "test_dataset = concatenate_datasets(test_batches)\n",
    "\n",
    "preprocessed_dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validate\": validate_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset.save_to_disk(\"../dataset/music_lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = DatasetDict.load_from_disk(\"../dataset/music_lib\")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the resulting dataset has six times as many entries as the original dataset due to\n",
    "the six spectrograms per song.\n",
    "\n",
    "To prove that everything worked as expected, we can again look at the first spectrogram of the train\n",
    "split..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_spectrum(tensor([preprocessed_dataset[\"train\"][0][model_input_name]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and see that everything is fine. Now we can move on to notebook\n",
    "[2. Augmentation](./2-augmentation.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
