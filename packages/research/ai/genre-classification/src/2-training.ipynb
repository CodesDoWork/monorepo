{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training\n",
    "\n",
    "In this step, we can go on to the actual training of the model.\n",
    "\n",
    "## 2.1 Preliminaries\n",
    "\n",
    "We first have to do some installations and imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate transformers[torch] torchaudio wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seaborn import heatmap\n",
    "from collections import Counter \n",
    "from datasets import DatasetDict\n",
    "from torch import tensor, Tensor\n",
    "from torch.nn import Module, Linear, LayerNorm\n",
    "from torch.nn.functional import softmax\n",
    "from torchaudio import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.nn.functional import interpolate, pad\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import ASTFeatureExtractor, ASTConfig, ASTForAudioClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I used Weights and Biases (wandb) to monitor the training process, we need to set the\n",
    "following environment variable to select the correct project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env WANDB_PROJECT=genre_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And login into my wandb instance. Sorry, no API key leaked here ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"...\", host=\"https://wandb.justinkonratt.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the genres and the dataset which have been uploaded from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/genres.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    genres = json.load(f)\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = DatasetDict.load_from_disk(\"./dataset/music_lib_balanced\")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The model\n",
    "\n",
    "First, load the model and set the column names for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "labels_name = \"labels\"\n",
    "paths_name = \"paths\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AST model works with an `ASTConfig`, which can be used to make changes to the model. Here we can\n",
    "set the number of classes `num_labels` and their respective names and IDs.\n",
    "\n",
    "During the course of this work the dropout values `hidden_dropout_prob` and\n",
    "`attention_probs_dropout_prob` as well as the temporal dimension of the input `max_length` have been\n",
    "modified. Changing `max_length` resulted in the loss of position embeddings and should thus be\n",
    "avoided.\n",
    "\n",
    "Changing the dropout didn't lead to a better performance as you can see in\n",
    "[notebook 6](./6-results.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "config.num_labels = len(genres)\n",
    "config.label2id = { genre: idx for idx, genre in enumerate(genres) }\n",
    "config.id2label = { idx: genre for idx, genre in enumerate(genres) }\n",
    "config.hidden_dropout_prob = 0.00\n",
    "config.attention_probs_dropout_prob = 0.00\n",
    "\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Metrics\n",
    "\n",
    "The following methods are used to calculate the metrics in the evaluation step. Since we are\n",
    "predicting the genre on every snippet, the important metric is the accuracy of the aggregated\n",
    "prediction. There are different aggregation methods which can be used: voting, weighted scoring, and\n",
    "using the mean or the max score of the predictions. They all are reported to see what works best.\n",
    "[The results](./6-results.ipynb) show that the `mean_pooling_score` works best as well as the\n",
    "`weighting_score`. Since `mean_pooling_score` has higher results at the end, it is used as primary\n",
    "performance metric.\n",
    "\n",
    "Typical classification metrics accuracy (per snippet), precision, recall, and f1 score are also\n",
    "reported. Furthermore, a confusion matrix is plotted for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aggregated_accuracy(predictions: Tensor, labels: Tensor):\n",
    "    v_score = 0\n",
    "    w_score = 0\n",
    "    a_score = 0\n",
    "    m_score = 0\n",
    "\n",
    "    songs_per_genre = [0 for _ in range(len(genres))]\n",
    "    correctly_predicted_per_genre = [0 for _ in range(len(genres))]\n",
    "\n",
    "    eval_subset = None\n",
    "    for subset in preprocessed_dataset.values():\n",
    "        if len(subset) == predictions.shape[0]:\n",
    "            eval_subset = subset\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    entries_per_song = list(Counter(eval_subset[paths_name]).values())\n",
    "    song_count = len(entries_per_song)\n",
    "    start_idx = 0\n",
    "    for song_entries in entries_per_song:\n",
    "        label = labels[start_idx]\n",
    "        true_labels.append(label)\n",
    "        songs_per_genre[label] += 1\n",
    "        song_logits = predictions[start_idx:start_idx + song_entries,:]\n",
    "        start_idx += song_entries\n",
    "\n",
    "        # voting\n",
    "        v_pred = song_logits.argmax(dim=1).mode().values.item()\n",
    "        if v_pred == label:\n",
    "            v_score += 1\n",
    "\n",
    "        # weighting\n",
    "        confidence = song_logits.softmax(dim=1).max(dim=1).values\n",
    "        weighted_logits = (song_logits.T * confidence).T\n",
    "        w_pred = weighted_logits.mean(dim=0).argmax().item()\n",
    "        if w_pred == label:\n",
    "            w_score += 1\n",
    "\n",
    "        # average\n",
    "        a_pred = song_logits.mean(dim=0).argmax().item()\n",
    "        predicted_labels.append(a_pred)\n",
    "        if a_pred == label:\n",
    "            a_score += 1\n",
    "            correctly_predicted_per_genre[label] += 1\n",
    "\n",
    "        # max\n",
    "        m_pred = song_logits.max(dim=0).values.argmax().item()\n",
    "        if m_pred == label:\n",
    "            m_score += 1\n",
    "\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    cm_normalized_row = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    heatmap(cm_normalized_row, xticklabels=genres, yticklabels=genres)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    threshold = 0.1\n",
    "    confusions = []\n",
    "    for i in range(cm_normalized_row.shape[0]):\n",
    "        for j in range(cm_normalized_row.shape[1]):\n",
    "            if i != j and cm_normalized_row[i, j] > threshold:\n",
    "                confusions.append((genres[i], genres[j], cm_normalized_row[i, j]))\n",
    "    confusion_df = pd.DataFrame(confusions, columns=['True Class', 'Predicted Class', 'Confusion Value'])\n",
    "    confusion_df = confusion_df.sort_values(by='Confusion Value', ascending=False)\n",
    "    print(confusion_df)\n",
    "\n",
    "    return {\n",
    "        \"voting_score\": v_score / song_count,\n",
    "        \"weighting_score\": w_score / song_count,\n",
    "        \"mean_pooling_score\": a_score / song_count,\n",
    "        \"max_pooling_score\": m_score / song_count,\n",
    "        **{ f\"{genre}_a_accuracy\": correctly_predicted_per_genre[idx] / songs_per_genre[idx] for idx, genre in enumerate(genres) }\n",
    "    }\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metrics = accuracy.compute(predictions=predictions, references=labels)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=labels, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=labels, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=labels, average=AVERAGE))\n",
    "    metrics.update(calc_aggregated_accuracy(tensor(logits), tensor(labels)))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Augmentation\n",
    "\n",
    "As for images, augmentation is said to be a good way to increase the robustness of the model for\n",
    "audio. It is used to add noise, remove some features, or change the length of the signal. This helps\n",
    "the model to learn different aspects of the data to perform the prediction and generalize better.\n",
    "For example, Frequency Masking helps the model to focus on different frequencies to learn important\n",
    "features from bass, mids, and highs equally. Noise and frequency shift can be added for better\n",
    "generalization and amplitude scaling allows the model to learn the task for different loudness\n",
    "levels. All these augmentations are directly applied to the spectrograms from the preprocessing.\n",
    "\n",
    "The only augmentation technique, which should not be used, is time stretching. Even a small stretch\n",
    "of +/-3 % led to unlearnable samples. As [\"Using Time Stretching\" in notebook 6](./6-results.ipynb)\n",
    "shows.\n",
    "\n",
    "The following augmentation pipeline is used while training with augmentation. All effects are\n",
    "applied with a probability of 50 %. [Results](./6-results.ipynb) have shown that more augmentation\n",
    "is better. That's why augmentation is applied to every sample instead to 0 % or 50 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecAugmentPipeline:\n",
    "    def __init__(\n",
    "            self,\n",
    "            p=0.5,\n",
    "            effects_p=0.5,\n",
    "            time_mask_param=30,\n",
    "            freq_mask_param=20,\n",
    "            noise_level=0.05,\n",
    "            stretch_range=(0.97, 1.03),\n",
    "            shift_range=3,\n",
    "            amplitude_range=(0.8, 1.2)\n",
    "    ):\n",
    "        self.p = p\n",
    "        self.effects_p = effects_p\n",
    "        self.time_mask = transforms.TimeMasking(time_mask_param)\n",
    "        self.freq_mask = transforms.FrequencyMasking(freq_mask_param)\n",
    "        self.noise_level = noise_level\n",
    "        self.stretch_range = stretch_range\n",
    "        self.shift_range = shift_range\n",
    "        self.amplitude_range = amplitude_range\n",
    "\n",
    "    def add_noise(self, spec):\n",
    "        return spec + torch.randn_like(spec) * self.noise_level\n",
    "    \n",
    "    def time_stretch(self, spec):\n",
    "        spec = spec.unsqueeze(0)\n",
    "        factor = random.uniform(*self.stretch_range)\n",
    "        new_steps = int(spec.size(-1) * factor)\n",
    "        new_spec = interpolate(spec, (spec.size(-2), new_steps), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "        return new_spec.resize_(spec.size(-3), spec.size(-2), spec.size(-1)) if factor >= 1 else pad(new_spec, (0, spec.size(-1) - new_steps))\n",
    "\n",
    "    def frequency_shift(self, spec):\n",
    "        shift = random.randint(-self.shift_range, self.shift_range)\n",
    "        return torch.roll(spec, shifts=shift, dims=-2)\n",
    "\n",
    "    def amplitude_scaling(self, spec):\n",
    "        return spec * random.uniform(*self.amplitude_range)\n",
    "\n",
    "    def __call__(self, spec):\n",
    "        if random.random() >= self.p:\n",
    "            return spec\n",
    "        \n",
    "        spec = spec.transpose(-1, -2)\n",
    "        if random.random() < self.effects_p:\n",
    "            spec = self.time_mask(spec)\n",
    "        \n",
    "        if random.random() < self.effects_p:\n",
    "            spec = self.freq_mask(spec)\n",
    "\n",
    "        if random.random() < self.effects_p:\n",
    "            spec = self.add_noise(spec)\n",
    "\n",
    "        # bad loss behavior\n",
    "        # if random.random() < self.effects_p:\n",
    "        #     spec = self.time_stretch(spec)\n",
    "\n",
    "        if random.random() < self.effects_p:\n",
    "            spec = self.frequency_shift(spec)\n",
    "\n",
    "        if random.random() < self.effects_p:\n",
    "            spec = self.amplitude_scaling(spec)\n",
    "\n",
    "        return spec.transpose(-1, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The augmentation pipeline is now instantiated and set as transform to the train split to apply it\n",
    "during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pipe = SpecAugmentPipeline(p=1)\n",
    "def augmentation(sample):\n",
    "    if model_input_name in sample:\n",
    "        sample[model_input_name] = aug_pipe(tensor(sample[model_input_name]))\n",
    "    return sample\n",
    "\n",
    "preprocessed_dataset[\"train\"].set_transform(augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Weighted Loss Function\n",
    "\n",
    "Due to the unbalanced dataset, one idea was to add weights to the loss function to add more focus on\n",
    "under-represented classes. The weights can be computed using the predefined `compute_class_weight`\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = preprocessed_dataset[\"train\"][labels_name]\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float, device=\"cuda\")\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can then be applied using a custom loss function with the trainer. Because we want to predict\n",
    "a single genre per song or snippet, the default `CrossEntropyLoss` is used in combination with the\n",
    "class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss_func(outputs, labels, num_items_in_batch):\n",
    "    logits = outputs.get(\"logits\")\n",
    "    loss_fct = CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
    "    return loss_fct(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Training\n",
    "\n",
    "For training, we use 1 GPU with a batch size of 64 because this fills the GPU best. Logs are sent\n",
    "to Weights and Biases. We use a default learning rate of `5e-5` and 5 fine-tuning epochs of which \n",
    "one tenth is warm up. Evaluation and save strategy is \"epoch\" to save and evaluate the model after every\n",
    "epoch while `log_steps` is set to 1 to report every training step. `fp16` is used because it reduces\n",
    "the memory used on the GPU due to lower precision of the floats while maintaining the same\n",
    "prediction accuracy.\n",
    "\n",
    "See the [results](./6-results.ipynb) to verify these decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "devices = 1\n",
    "warmup_epochs = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/ast_classifier\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"some_name_10\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.00,\n",
    "    warmup_steps=round(len(preprocessed_dataset[\"train\"]) / batch_size * warmup_epochs),\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=batch_size // devices,\n",
    "    per_device_eval_batch_size=batch_size // devices,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mean_pooling_score\",\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    fp16=True,\n",
    "    save_total_limit=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the trainer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_dataset[\"train\"],\n",
    "    eval_dataset=preprocessed_dataset[\"validate\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    # compute_loss_func=weighted_loss_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can evaluate the results using the `evaluate` function with `test` as metric key\n",
    "prefix to log them correctly to `wandb` and finish the run with `wandb.finish()`. To know which\n",
    "model is used, we can print the `trainer.state.best_model_checkpoint`.\n",
    "\n",
    "The confusion matrices and the dataframes show the confusions of the model. Don't look at them in\n",
    "too much detail, we will dive deeper into them for better runs the song prediction notebook. This\n",
    "run was made with a balanced dataset, of which you can see the creation in the\n",
    "[next notebook](./3-balanced-dataset.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.state.best_model_checkpoint)\n",
    "results = trainer.evaluate(preprocessed_dataset[\"test\"], metric_key_prefix=\"test\")\n",
    "print(results)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
