{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Song Dataset\n",
    "\n",
    "This notebook creates a new dataset where entries are songs and not snippets to classify a whole\n",
    "song using its six snippets at once.\n",
    "\n",
    "You can look at the training process here: [5. Training Songs](./5-training_songs.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate transformers[torch] torchaudio wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch import tensor\n",
    "from datasets import DatasetDict, Dataset, concatenate_datasets, Features, Value, ClassLabel, Array3D\n",
    "from transformers import ASTFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/genres.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    genres = json.load(f)\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "labels_name = \"labels\"\n",
    "paths_name = \"paths\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = DatasetDict.load_from_disk(\"./dataset/music_lib\")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_song = 6\n",
    "\n",
    "song_features = Features({\n",
    "    model_input_name: Array3D(shape=(samples_per_song, feature_extractor.max_length, feature_extractor.num_mel_bins), dtype=\"float32\"),\n",
    "    labels_name: ClassLabel(names=genres),\n",
    "    paths_name: Value(dtype=\"string\")\n",
    "})\n",
    "\n",
    "for split, dataset in preprocessed_dataset.items():\n",
    "    batch_size = 250\n",
    "    batch_spectrograms = []\n",
    "    batch_labels = []\n",
    "    batch_paths = []\n",
    "    def save_batch(idx: int):\n",
    "        batch_dict = {model_input_name: batch_spectrograms, labels_name: batch_labels, paths_name: batch_paths}\n",
    "        partial_dataset = Dataset.from_dict(batch_dict, features=song_features)\n",
    "        partial_dataset.save_to_disk(f\"./songs/{split}_batch_{idx}\")\n",
    "\n",
    "    idx = 0\n",
    "    for i in tqdm(range(0, len(dataset), samples_per_song), desc=f\"Processing {split} samples\", total=len(dataset) // samples_per_song):\n",
    "        batch_spectrograms.append(dataset[i:i + samples_per_song][\"input_values\"])\n",
    "        batch_labels.append(dataset[i][\"labels\"])\n",
    "        batch_paths.append(dataset[i][\"paths\"])\n",
    "\n",
    "        if len(batch_spectrograms) == batch_size:\n",
    "            save_batch(idx)\n",
    "            batch_spectrograms = []\n",
    "            batch_labels = []\n",
    "            batch_paths = []\n",
    "            idx += 1\n",
    "    save_batch(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_paths = [f\"./songs/{batch_dir}\" for batch_dir in os.listdir(\"./songs\")]\n",
    "\n",
    "train_batches = [Dataset.load_from_disk(path) for path in batch_paths if \"train\" in path]\n",
    "validate_batches = [Dataset.load_from_disk(path) for path in batch_paths if \"validate\" in path]\n",
    "test_batches = [Dataset.load_from_disk(path) for path in batch_paths if \"test\" in path]\n",
    "\n",
    "train_dataset = concatenate_datasets(train_batches)\n",
    "validate_dataset = concatenate_datasets(validate_batches)\n",
    "test_dataset = concatenate_datasets(test_batches)\n",
    "\n",
    "song_dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validate\": validate_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "song_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_dataset.save_to_disk(\"./dataset/music_lib_songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(song_dataset[\"train\"][0][model_input_name]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
