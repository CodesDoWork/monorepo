{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training Songs\n",
    "\n",
    "This notebook aims to add a different classification head to the AST model to train with all six\n",
    "snippets of a song. The training and parameters are the same as in the previous training notebook,\n",
    "thus only the model modifications and the results will be analyzed in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate transformers[torch] torchaudio wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from seaborn import heatmap\n",
    "from datasets import DatasetDict\n",
    "from torch import tensor, Tensor, matmul\n",
    "from torch.nn import Module, Linear, LayerNorm, Embedding\n",
    "from torch.nn.functional import softmax\n",
    "from torchaudio import transforms\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.nn.functional import interpolate, pad\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import ASTFeatureExtractor, ASTConfig, ASTForAudioClassification, TrainingArguments, Trainer\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env WANDB_PROJECT=genre_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"...\", host=\"https://wandb.justinkonratt.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/genres.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    genres = json.load(f)\n",
    "genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the song dataset instead of the default one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = DatasetDict.load_from_disk(\"./dataset/music_lib_songs\")\n",
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "labels_name = \"labels\"\n",
    "paths_name = \"paths\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "config.num_labels = len(genres)\n",
    "config.label2id = { genre: idx for idx, genre in enumerate(genres) }\n",
    "config.id2label = { idx: genre for idx, genre in enumerate(genres) }\n",
    "config.hidden_dropout_prob = 0.00\n",
    "config.attention_probs_dropout_prob = 0.00\n",
    "\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "A few modifications have been tested here:\n",
    "\n",
    "1. Adding temporal attention to the model and weight the snippet CLS tokens to obtain the song CLS.\n",
    "2. Using the means of the six CLS tokens of the snippets instead of weighting them.\n",
    "3. Adding six position embeddings (one to each snippet CLS) and use temporal attention.\n",
    "4. Add a CLS-embedding and seven position embeddings, then use simple self attention and use the\n",
    "   resulting CLS token as the song's CLS token.\n",
    "\n",
    "For this to work, an own model is used to wrap AST. Additionally, the classifier of the model is\n",
    "changed by a custom torch module. The batch size is reduced during training to fit on the GPU. 12\n",
    "was chosen because 12 * 6 = 72, which is the number of snippets per batch. Thus, the batch has a\n",
    "shape of (12, 6, 1024, 128). To give it into the model, the batch is flattened to (72, 1024, 128),\n",
    "such that each snippet is processed separately. After that, the original shape is restored for the\n",
    "classification.\n",
    "\n",
    "Despite all results seemed quite good, taking the mean of the CLS tokens worked but. But none of\n",
    "those methods was better than classifying each snippet separately and aggregating the classification\n",
    "results. [See here](./6-results.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(Module):\n",
    "    def __init__(self):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.attention_weights = Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, cls_tokens):\n",
    "        scores = self.attention_weights(cls_tokens)\n",
    "        attention_weights = softmax(scores, dim=-2)\n",
    "        return torch.sum(attention_weights * cls_tokens, dim=-2)\n",
    "    \n",
    "class SelfAttention(Module):\n",
    "    def __init__(self):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.h_sqrt = config.hidden_size ** 0.5\n",
    "        self.query = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.key = Linear(config.hidden_size, config.hidden_size)\n",
    "        self.value = Linear(config.hidden_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, cls_tokens):\n",
    "        Q = self.query(cls_tokens)\n",
    "        K = self.key(cls_tokens)\n",
    "        V = self.value(cls_tokens)\n",
    "        scores = matmul(Q, K.transpose(-1, -2)) / self.h_sqrt\n",
    "        attention_weights = softmax(scores, dim=-2)\n",
    "        return cls_tokens + matmul(attention_weights, V)\n",
    "\n",
    "class AggregateSnipptesClassifier(Module):\n",
    "    def __init__(self, snippets_per_song: int):\n",
    "        super(AggregateSnipptesClassifier, self).__init__()\n",
    "        self.snippets_per_song = snippets_per_song\n",
    "        self.layernorm = LayerNorm(normalized_shape=(config.hidden_size,), eps=1e-12, elementwise_affine=True)\n",
    "        # self.position_embeddings = Embedding(1 + snippets_per_song, config.hidden_size)\n",
    "        # self.cls_embedding = Embedding(1, config.hidden_size)\n",
    "        self.attention = TemporalAttention() # SelfAttention()\n",
    "        self.classifier = Linear(in_features=config.hidden_size, out_features=config.num_labels, bias=True)\n",
    "\n",
    "    def forward(self, cls_tokens: Tensor):\n",
    "        cls_tokens = cls_tokens.reshape(cls_tokens.shape[0] // self.snippets_per_song, self.snippets_per_song, cls_tokens.shape[1])\n",
    "        normalized_cls_tokens = self.layernorm(cls_tokens)\n",
    "        # cls_token = self.cls_embedding(torch.arange(1, device=cls_tokens.device)).expand(cls_tokens.shape[0], 1, cls_tokens.shape[2])\n",
    "        # cls_tokens = torch.cat([cls_token, normalized_cls_tokens], dim=1)\n",
    "        song_cls_token = self.attention(normalized_cls_tokens) # normalized_cls_tokens.mean(dim=-2)\n",
    "        # song_cls_tokens = self.attention(cls_tokens) # self.attention(self.position_embeddings(torch.arange(self.snippets_per_song, device=cls_tokens.device)) + normalized_cls_tokens)\n",
    "        return self.classifier(song_cls_token)\n",
    "        # return self.classifier(song_cls_tokens[:,0,:])\n",
    "\n",
    "class ASTSnippetModel(Module):\n",
    "    def __init__(self, snippets_per_song: int = 6):\n",
    "        super(ASTSnippetModel, self).__init__()\n",
    "        self.model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)\n",
    "        self.model.classifier = AggregateSnipptesClassifier(snippets_per_song)\n",
    "        self.loss = CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_values: Tensor, labels: Tensor):\n",
    "        # input_values shape: (batch_size, snippets_per_song, time, freq)\n",
    "        batch = input_values.reshape(input_values.shape[0] * input_values.shape[1], input_values.shape[2], input_values.shape[3])\n",
    "        logits = self.model(batch).logits\n",
    "        loss = self.loss(logits, labels)\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "model = ASTSnippetModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Metrics\n",
    "\n",
    "We don't need all the aggregated metrics here because we already have song predictions. Thus, the\n",
    "main metric is now `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aggregated_accuracy(predictions: Tensor, labels: Tensor):\n",
    "    predicted_labels = predictions.argmax(dim=1)\n",
    "    cm = confusion_matrix(labels, predicted_labels)\n",
    "    cm_normalized_row = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]    \n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    heatmap(cm_normalized_row, xticklabels=genres, yticklabels=genres)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    threshold = 0.1\n",
    "    confusions = []\n",
    "    for i in range(cm_normalized_row.shape[0]):\n",
    "        for j in range(cm_normalized_row.shape[1]):\n",
    "            if i != j and cm_normalized_row[i, j] > threshold:\n",
    "                confusions.append((genres[i], genres[j], cm_normalized_row[i, j]))\n",
    "    confusion_df = pd.DataFrame(confusions, columns=['True Class', 'Predicted Class', 'Confusion Value'])\n",
    "    confusion_df = confusion_df.sort_values(by='Confusion Value', ascending=False)\n",
    "    print(confusion_df)\n",
    "\n",
    "    correctly_predicted_per_genre = [0] * len(genres)\n",
    "    songs_per_genre = [0] * len(genres)\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == predicted_labels[idx]:\n",
    "            correctly_predicted_per_genre[label] += 1\n",
    "        songs_per_genre[label] += 1\n",
    "\n",
    "    return {\n",
    "        **{ f\"{genre}_accuracy\": correctly_predicted_per_genre[idx] / songs_per_genre[idx] for idx, genre in enumerate(genres) }\n",
    "    }\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    metrics = accuracy.compute(predictions=predictions, references=labels)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=labels, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=labels, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=labels, average=AVERAGE))\n",
    "    metrics.update(calc_aggregated_accuracy(tensor(logits), tensor(labels)))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecAugmentPipeline:\n",
    "    def __init__(\n",
    "            self,\n",
    "            p=0.5,\n",
    "            effects_p=0.5,\n",
    "            time_mask_param=30,\n",
    "            freq_mask_param=20,\n",
    "            noise_level=0.05,\n",
    "            stretch_range=(0.97, 1.03),\n",
    "            shift_range=3,\n",
    "            amplitude_range=(0.8, 1.2)\n",
    "    ):\n",
    "        self.p = p\n",
    "        self.effects_p = effects_p\n",
    "        self.time_mask = transforms.TimeMasking(time_mask_param)\n",
    "        self.freq_mask = transforms.FrequencyMasking(freq_mask_param)\n",
    "        self.noise_level = noise_level\n",
    "        self.stretch_range = stretch_range\n",
    "        self.shift_range = shift_range\n",
    "        self.amplitude_range = amplitude_range\n",
    "\n",
    "    def add_noise(self, spec):\n",
    "        return spec + torch.randn_like(spec) * self.noise_level\n",
    "    \n",
    "    def time_stretch(self, spec):\n",
    "        spec = spec.unsqueeze(0)\n",
    "        factor = random.uniform(*self.stretch_range)\n",
    "        new_steps = int(spec.size(-1) * factor)\n",
    "        new_spec = interpolate(spec, (spec.size(-2), new_steps), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "        return new_spec.resize_(spec.size(-3), spec.size(-2), spec.size(-1)) if factor >= 1 else pad(new_spec, (0, spec.size(-1) - new_steps))\n",
    "\n",
    "    def frequency_shift(self, spec):\n",
    "        shift = random.randint(-self.shift_range, self.shift_range)\n",
    "        return torch.roll(spec, shifts=shift, dims=-2)\n",
    "\n",
    "    def amplitude_scaling(self, spec):\n",
    "        return spec * random.uniform(*self.amplitude_range)\n",
    "\n",
    "    def __call__(self, spec):\n",
    "        if random.random() >= self.p:\n",
    "            return spec\n",
    "        \n",
    "        spec = spec.transpose(-1, -2)\n",
    "        if random.random() < self.effects_p:\n",
    "            spec = self.time_mask(spec)\n",
    "        \n",
    "        if random.random() < self.effects_p:\n",
    "            spec = self.freq_mask(spec)\n",
    "\n",
    "        if random.random() < self.effects_p:\n",
    "            spec = self.add_noise(spec)\n",
    "\n",
    "        # bad loss behavior\n",
    "        # if random.random() < self.effects_p:\n",
    "        #     spec = self.time_stretch(spec)\n",
    "\n",
    "        if random.random() < self.effects_p:\n",
    "            spec = self.frequency_shift(spec)\n",
    "\n",
    "        if random.random() < self.effects_p:\n",
    "            spec = self.amplitude_scaling(spec)\n",
    "\n",
    "        return spec.transpose(-1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_pipe = SpecAugmentPipeline(p=1)\n",
    "def augmentation(sample):\n",
    "    if model_input_name in sample:\n",
    "        sample[model_input_name] = aug_pipe(tensor(sample[model_input_name]))\n",
    "    return sample\n",
    "\n",
    "preprocessed_dataset[\"train\"].set_transform(augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "devices = 1\n",
    "epochs = 5\n",
    "warmup_epochs = epochs / 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs/ast_classifier\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"songs_self_attn\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.00,\n",
    "    warmup_steps=round(len(preprocessed_dataset[\"train\"]) / batch_size * warmup_epochs),\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size // devices,\n",
    "    per_device_eval_batch_size=batch_size // devices,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    fp16=True,\n",
    "    save_total_limit=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocessed_dataset[\"train\"],\n",
    "    eval_dataset=preprocessed_dataset[\"validate\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the temporal attention\n",
    "\n",
    "This training shows the best results from the training with temporal weighting of the snippets'\n",
    "CLS-Tokens. On each evaluation, a confusion matrix and the most common confusions are plotted.\n",
    "You can clearly see, that the highly represented classes are the most predicted ones in the\n",
    "beginning, but the model improves over time and also learns the other genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set, we can see below which genres are often confused. For example, Folk is all the time\n",
    "classified as Country, which is quite similar. Also, Epic and Lo-Fi are classified as Chillstep\n",
    "half of the time. Again a very similar Genre. Thus, we can see that the model has learned important\n",
    "characteristics of the music, but still lacks to focus on smaller aspects. Maybe spectrograms are\n",
    "not the best feature for this task. Also, the dataset is quite small and opinionated, which further\n",
    "decreases the model's ability to learn from it. The test recall shows a score of around 70 %, making\n",
    "it a rather bad model for this task, although the accuracy is around 86.2 %. But this is mainly\n",
    "because of the highly represented genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.state.best_model_checkpoint)\n",
    "results = trainer.evaluate(preprocessed_dataset[\"test\"], metric_key_prefix=\"test\")\n",
    "print(results)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
